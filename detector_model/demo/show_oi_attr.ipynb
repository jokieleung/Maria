{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'detectron2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c663f40b1a21>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdetectron2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# import some common detectron2 utilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'detectron2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "\n",
    "import detectron2\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "# Show the image in ipynb\n",
    "from IPython.display import clear_output, Image, display\n",
    "import PIL.Image\n",
    "def showarray(a, fmt='jpeg'):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    f = io.BytesIO()\n",
    "    PIL.Image.fromarray(a).save(f, fmt)\n",
    "    display(Image(data=f.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VG Classes\n",
    "data_path = 'data/genome/1600-400-20'\n",
    "\n",
    "vg_classes = []\n",
    "with open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n",
    "    for object in f.readlines():\n",
    "        vg_classes.append(object.split(',')[0].lower().strip())\n",
    "        \n",
    "vg_attrs = []\n",
    "with open(os.path.join(data_path, 'attributes_vocab.txt')) as f:\n",
    "    for object in f.readlines():\n",
    "        vg_attrs.append(object.split(',')[0].lower().strip())\n",
    "\n",
    "\n",
    "MetadataCatalog.get(\"vg\").thing_classes = vg_classes\n",
    "MetadataCatalog.get(\"vg\").attr_classes = vg_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(\"../configs/VG-Detection/faster_rcnn_R_101_C4_attr_caffemaxpool.yaml\")\n",
    "cfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\n",
    "cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
    "# VG Weight\n",
    "cfg.MODEL.WEIGHTS = \"http://nlp.cs.unc.edu/models/faster_rcnn_from_caffe_attr.pkl\"\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = cv2.imread(\"data/images/input.jpg\")\n",
    "\n",
    "im = cv2.imread(\"/data/data_for_vokenization/open_images/images/b46de73fbd9ddba2.jpg\")\n",
    "im_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "showarray(im_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_OBJECTS = 36\n",
    "\n",
    "from detectron2.modeling.postprocessing import detector_postprocess\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n",
    "\n",
    "def doit(raw_image):\n",
    "    with torch.no_grad():\n",
    "        raw_height, raw_width = raw_image.shape[:2]\n",
    "        print(\"Original image size: \", (raw_height, raw_width))\n",
    "        \n",
    "        # Preprocessing\n",
    "        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n",
    "        print(\"Transformed image size: \", image.shape[:2])\n",
    "        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n",
    "        images = predictor.model.preprocess_image(inputs)\n",
    "        \n",
    "        # Run Backbone Res1-Res4\n",
    "        features = predictor.model.backbone(images.tensor)\n",
    "        \n",
    "        # Generate proposals with RPN\n",
    "        proposals, _ = predictor.model.proposal_generator(images, features, None)\n",
    "        proposal = proposals[0]\n",
    "        print('Proposal Boxes size:', proposal.proposal_boxes.tensor.shape)\n",
    "        \n",
    "        # Run RoI head for each proposal (RoI Pooling + Res5)\n",
    "        proposal_boxes = [x.proposal_boxes for x in proposals]\n",
    "        features = [features[f] for f in predictor.model.roi_heads.in_features]\n",
    "        box_features = predictor.model.roi_heads._shared_roi_transform(\n",
    "            features, proposal_boxes\n",
    "        )\n",
    "        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n",
    "        print('Pooled features size:', feature_pooled.shape)\n",
    "        \n",
    "        # Predict classes and boxes for each proposal.\n",
    "        pred_class_logits, pred_attr_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n",
    "        outputs = FastRCNNOutputs(\n",
    "            predictor.model.roi_heads.box2box_transform,\n",
    "            pred_class_logits,\n",
    "            pred_proposal_deltas,\n",
    "            proposals,\n",
    "            predictor.model.roi_heads.smooth_l1_beta,\n",
    "        )\n",
    "        probs = outputs.predict_probs()[0]\n",
    "        boxes = outputs.predict_boxes()[0]\n",
    "        \n",
    "        attr_prob = pred_attr_logits[..., :-1].softmax(-1)\n",
    "        max_attr_prob, max_attr_label = attr_prob.max(-1)\n",
    "        \n",
    "        # Note: BUTD uses raw RoI predictions,\n",
    "        #       we use the predicted boxes instead.\n",
    "        # boxes = proposal_boxes[0].tensor    \n",
    "        \n",
    "        # NMS\n",
    "        for nms_thresh in np.arange(0.5, 1.0, 0.1):\n",
    "            instances, ids = fast_rcnn_inference_single_image(\n",
    "                boxes, probs, image.shape[1:], \n",
    "                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS\n",
    "            )\n",
    "            if len(ids) == NUM_OBJECTS:\n",
    "                break\n",
    "                \n",
    "        instances = detector_postprocess(instances, raw_height, raw_width)\n",
    "        roi_features = feature_pooled[ids].detach()\n",
    "        max_attr_prob = max_attr_prob[ids].detach()\n",
    "        max_attr_label = max_attr_label[ids].detach()\n",
    "        instances.attr_scores = max_attr_prob\n",
    "        instances.attr_classes = max_attr_label\n",
    "        \n",
    "        print(instances)\n",
    "        \n",
    "        return instances, roi_features\n",
    "    \n",
    "instances, features = doit(im)\n",
    "\n",
    "# print(instances.pred_boxes)\n",
    "# print(instances.scores)\n",
    "# print(instances.pred_classes)\n",
    "# print(instances.attr_classes)\n",
    "# print(instances.attr_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the boxes, labels, and features\n",
    "# pred = instances.to('cpu')\n",
    "# v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n",
    "# v = v.draw_instance_predictions(pred)\n",
    "# showarray(v.get_image()[:, :, ::-1])\n",
    "# print('instances:\\n', instances)\n",
    "# print()\n",
    "# print('boxes:\\n', instances.pred_boxes)\n",
    "# print()\n",
    "# print('Shape of features:\\n', features.shape)\n",
    "\n",
    "# Show the boxes, labels, and features\n",
    "pred = instances.to('cpu')\n",
    "v = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\n",
    "v = v.draw_instance_predictions(pred)\n",
    "showarray(v.get_image()[:, :, ::-1])\n",
    "print('instances:\\n', instances)\n",
    "print()\n",
    "# print('boxes:\\n', instances.pred_boxes)\n",
    "# print()\n",
    "\n",
    "print('pred_scores\\n', instances.scores)\n",
    "print()\n",
    "\n",
    "print('pred_classes\\n', instances.pred_classes)\n",
    "print()\n",
    "instances.pred_classes = instances.pred_classes.to('cpu')\n",
    "class_label = [MetadataCatalog.get(\"vg\").thing_classes[i] for i in instances.pred_classes]\n",
    "print('class_label\\n', class_label)\n",
    "print()\n",
    "\n",
    "no_repeat_class_label = list(set(class_label))\n",
    "print('no_repeat_class_label\\n', no_repeat_class_label)\n",
    "print()\n",
    "\n",
    "print('attr_classes\\n', instances.attr_classes)\n",
    "print()\n",
    "instances.attr_classes = instances.attr_classes.to('cpu')\n",
    "attr_classes = [MetadataCatalog.get(\"vg\").attr_classes[i] for i in instances.attr_classes]\n",
    "attr_classes = list(set(attr_classes))\n",
    "print('attr_label\\n', attr_classes)\n",
    "print()\n",
    "\n",
    "print('Shape of features:\\n', features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the correspondence of RoI features\n",
    "pred_class_logits, pred_attr_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(features)\n",
    "pred_class_probs = torch.nn.functional.softmax(pred_class_logits, -1)[:, :-1]\n",
    "max_probs, max_classes = pred_class_probs.max(-1)\n",
    "print(\"%d objects are different, it is because the classes-aware NMS process\" % (NUM_OBJECTS - torch.eq(instances.pred_classes, max_classes).sum().item()))\n",
    "print(\"The total difference of score is %0.4f\" % (instances.scores - max_probs).abs().sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
